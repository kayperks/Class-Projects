{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "Project_1_CS_634_Task2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6IkUL6vLLtZ"
      },
      "source": [
        "# Project 1 - Task #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGlR2xZOLLtd"
      },
      "source": [
        "This project will reference the Multi-Domain Sentiment Dataset that is availible here :\n",
        "https://www.cs.jhu.edu/~mdredze/datasets/sentiment/ \n",
        "\n",
        "The Multi-Domain Sentiment Dataset contains product reviews from Amazon.com from 4 product types (domains): Kitchen, Books, DVDs, and Electronics. \n",
        "For each domain, there are several thousand reviews. Reviews can range from 1 to 5 stars, and can be converted into binary labels if needed.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDOEebYQLLtd"
      },
      "source": [
        "Import necessary libraries :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T11:20:01.269017Z",
          "start_time": "2018-04-07T11:20:01.255673Z"
        },
        "id": "e0Du_OzgLLte",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89048dfb-4944-4a4e-b3ba-81dfafb5ee64"
      },
      "source": [
        "from six.moves import urllib\n",
        "import os\n",
        "import tarfile\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(['punkt','wordnet','stopwords'])\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsfX6XphLLtf"
      },
      "source": [
        "Download the required file and extract to our working directory :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:50:03.095310Z",
          "start_time": "2018-04-07T10:50:03.065836Z"
        },
        "id": "4StIop9MLLtf"
      },
      "source": [
        "download_root = \"https://www.cs.jhu.edu/~mdredze/datasets/sentiment/\"\n",
        "file_name = \"domain_sentiment_data.tar.gz\"\n",
        "sentiment_url =  download_root + file_name\n",
        "\n",
        "def download_extract(url,location):\n",
        "    '''\n",
        "    url: ulr location where the data resides\n",
        "    location: location on workstation the data needs to be copied to\n",
        "    '''\n",
        "    gz_path =  os.path.join(location,file_name)\n",
        "    _ = urllib.request.urlretrieve(url = sentiment_url,filename=gz_path)\n",
        "    gz_folder = tarfile.open(gz_path)\n",
        "    gz_folder.extractall(path=location)\n",
        "    gz_folder.close()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:22.130841Z",
          "start_time": "2018-04-07T10:50:03.098631Z"
        },
        "id": "EsJBaYwBLLtg"
      },
      "source": [
        "download_extract(url= sentiment_url,location=os.getcwd())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:22.280131Z",
          "start_time": "2018-04-07T10:51:22.134124Z"
        },
        "id": "KZc7dMGjLLtg"
      },
      "source": [
        "stopwords = stopwords.words('english')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-05T15:42:09.033172Z",
          "start_time": "2018-04-05T15:42:09.021039Z"
        },
        "id": "lAgx6uu8LLtg"
      },
      "source": [
        "stopwords that are too restrictive, and will be removed :\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:22.289543Z",
          "start_time": "2018-04-07T10:51:22.283824Z"
        },
        "id": "6rT_1vgRLLth"
      },
      "source": [
        "not_stopwords= ['best','better','good','great',\n",
        "'greater','greatest','important','interesting','problem','problems','work',\n",
        "'worked','working','works','would']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:22.297659Z",
          "start_time": "2018-04-07T10:51:22.292881Z"
        },
        "id": "nTaqKTaULLth"
      },
      "source": [
        "stopwords = [x for x in stopwords if x not in not_stopwords]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-02T12:47:28.890250Z",
          "start_time": "2018-04-02T12:47:28.885622Z"
        },
        "id": "FEhtNsnyLLth"
      },
      "source": [
        "The reviews are in a XML format, the BeautifulSoup library will assit with importing the file :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:23.069182Z",
          "start_time": "2018-04-07T10:51:22.300870Z"
        },
        "id": "7hc5udapLLth"
      },
      "source": [
        "positive_reviews = (BeautifulSoup(open('sorted_data_acl/electronics/positive.review')\n",
        "                                  .read(),'lxml'))\n",
        "positive_reviews = positive_reviews.findAll('review_text')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:23.915722Z",
          "start_time": "2018-04-07T10:51:23.071436Z"
        },
        "id": "CIb5AogTLLti"
      },
      "source": [
        "negative_reviews = (BeautifulSoup(open('sorted_data_acl/electronics/negative.review')\n",
        "                                  .read(),'lxml'))\n",
        "negative_reviews = negative_reviews.findAll('review_text')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:23.925120Z",
          "start_time": "2018-04-07T10:51:23.918611Z"
        },
        "id": "5bSiHxAKLLti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d0a6e0-d440-4450-9f44-337b69f8f894"
      },
      "source": [
        "print('Number of positive reviews: {}'.format(len(positive_reviews)))\n",
        "print('Number of negative reviews: {}'.format(len(negative_reviews)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive reviews: 1000\n",
            "Number of negative reviews: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw7XdQxnLLti"
      },
      "source": [
        "Notice the reviews are evenly split. Now we'll create a function to assist  with with tokenizing the reviews by first converting the text to lowercase, keeping words whose length is greater than 2, lemmatization to return the base or dictionary form of a word, and lastly to remove stopwords. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:23.944889Z",
          "start_time": "2018-04-07T10:51:23.928813Z"
        },
        "id": "COncAAVpLLtj"
      },
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def my_tokenizer(s):\n",
        "    s = s.lower()\n",
        "    tokens = nltk.tokenize.word_tokenize(s)\n",
        "    tokens = [t for t in tokens if len(t) > 2 ]\n",
        "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
        "    tokens = [t for t in tokens if t not in  stopwords]\n",
        "    return tokens"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xd4zRBHLLtj"
      },
      "source": [
        "In this section, we'll create a word to index map, this will assist with storing the word-frequency vectors. \n",
        "We'll also save the tokenized versions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:32.031935Z",
          "start_time": "2018-04-07T10:51:23.948617Z"
        },
        "id": "dStqIIu_LLtj"
      },
      "source": [
        "word_index_map = {}\n",
        "current_index = 0\n",
        "\n",
        "positive_tokenize = []\n",
        "nagative_tokenize = []\n",
        "\n",
        "\n",
        "for review in positive_reviews:\n",
        "    tokens = my_tokenizer(review.text)\n",
        "    positive_tokenize.append(tokens)\n",
        "\n",
        "    for token in tokens:\n",
        "        if token not in word_index_map:\n",
        "            word_index_map[token] = current_index\n",
        "            current_index += 1\n",
        "            \n",
        "            \n",
        "for review in negative_reviews:\n",
        "    tokens = my_tokenizer(review.text)\n",
        "    nagative_tokenize.append(tokens)\n",
        "\n",
        "    for token in tokens:\n",
        "        if token not in word_index_map:\n",
        "            word_index_map[token] = current_index\n",
        "            current_index += 1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:32.050146Z",
          "start_time": "2018-04-07T10:51:32.034338Z"
        },
        "id": "yQ0I43HVLLtj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c4214b1-c913-4f9f-f8e7-65d360a55e7c"
      },
      "source": [
        "positive_tokenize[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['purchased',\n",
              " 'unit',\n",
              " 'due',\n",
              " 'frequent',\n",
              " 'blackout',\n",
              " 'area',\n",
              " 'power',\n",
              " 'supply',\n",
              " 'going',\n",
              " 'bad',\n",
              " 'run',\n",
              " 'cable',\n",
              " 'modem',\n",
              " 'router',\n",
              " 'lcd',\n",
              " 'monitor',\n",
              " 'minute',\n",
              " 'enough',\n",
              " 'time',\n",
              " 'save',\n",
              " 'work',\n",
              " 'shut',\n",
              " 'equally',\n",
              " 'important',\n",
              " 'know',\n",
              " 'electronics',\n",
              " 'receiving',\n",
              " 'clean',\n",
              " 'power',\n",
              " 'feel',\n",
              " 'investment',\n",
              " 'minor',\n",
              " 'compared',\n",
              " 'loss',\n",
              " 'valuable',\n",
              " 'data',\n",
              " 'failure',\n",
              " 'equipment',\n",
              " 'due',\n",
              " 'power',\n",
              " 'spike',\n",
              " 'irregular',\n",
              " 'power',\n",
              " 'supply',\n",
              " 'always',\n",
              " 'amazon',\n",
              " 'business',\n",
              " 'day']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4g1e7NhLLtk"
      },
      "source": [
        "In this section, we'll create a function to calculate the word frequency per review and calculate the proportion of times a word appears in a particular review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:32.069765Z",
          "start_time": "2018-04-07T10:51:32.052854Z"
        },
        "id": "heH3tfAOLLtk"
      },
      "source": [
        "def tokens_to_vector(tokens,label):\n",
        "    x = np.zeros(len(word_index_map)+1)\n",
        "    for t in tokens:\n",
        "        i = word_index_map[t]\n",
        "        x[i] += 1\n",
        "    x = x/ x.sum()\n",
        "    x[-1] = label\n",
        "    return x\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:32.398200Z",
          "start_time": "2018-04-07T10:51:32.074341Z"
        },
        "id": "7DZEt_ZfLLtk"
      },
      "source": [
        "N = len(positive_reviews) + len(negative_reviews)\n",
        "data = np.zeros((N,len(word_index_map)+1))\n",
        "i = 0\n",
        "\n",
        "for token in positive_tokenize:\n",
        "    xy = tokens_to_vector(token,1)\n",
        "    data[i,:] = xy\n",
        "    i += 1\n",
        "    \n",
        "for token in nagative_tokenize:\n",
        "    xy = tokens_to_vector(token,0)\n",
        "    data[i,:] = xy\n",
        "    i += 1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgtbm0N2LLtk"
      },
      "source": [
        "Shuffle the data and split it into a training / test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:32.499945Z",
          "start_time": "2018-04-07T10:51:32.400496Z"
        },
        "id": "ttK4e6YTLLtk"
      },
      "source": [
        "np.random.seed(567)\n",
        "np.random.shuffle(data)\n",
        "\n",
        "X = data[:,:-1]\n",
        "y = data[:,-1]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:32.507838Z",
          "start_time": "2018-04-07T10:51:32.502196Z"
        },
        "id": "mE6278-VLLtl"
      },
      "source": [
        "X_train = X[:-100,]\n",
        "y_train = y[:-100]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:32.515189Z",
          "start_time": "2018-04-07T10:51:32.510930Z"
        },
        "id": "-O0awBXELLtl"
      },
      "source": [
        "X_test = X[-100:,]\n",
        "y_test = y[-100:]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b5rMrXWYy11"
      },
      "source": [
        "Logistic Regression : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UU0QzvnYanK",
        "outputId": "cd4ce37d-2e71-4397-ed1d-032b854fb41e"
      },
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train,y_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGJiev_eYecC",
        "outputId": "06ceef98-1fad-48d0-c092-3b73c207e7c6"
      },
      "source": [
        "print('Accuracy rate {:1.2f} '.format(model.score(X_test,y_test)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy rate 0.73 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyeJDEbYYlmt"
      },
      "source": [
        "Random Forest :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:55:42.827364Z",
          "start_time": "2018-04-07T10:55:42.823495Z"
        },
        "id": "29j-ME3dLLtn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97b7445-1fe8-4ed2-a328-97f5f4e8a3bf"
      },
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=200,random_state=893)\n",
        "rf_model.fit(X_train,y_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
              "                       n_jobs=None, oob_score=False, random_state=893,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:56:11.727943Z",
          "start_time": "2018-04-07T10:56:11.685927Z"
        },
        "id": "j0ilU-5ALLto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9139d756-5b7f-48c8-cf67-ed0672922567"
      },
      "source": [
        "print('Accuracy rate {:1.2f} '.format(rf_model.score(X_test,y_test)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy rate 0.80 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3_IoWRXLLtm"
      },
      "source": [
        "Code to review the weights for different words. A threshold of 0.5 will be used. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-07T10:51:33.600940Z",
          "start_time": "2018-04-07T10:51:33.550061Z"
        },
        "id": "5-l9VwQZLLtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded1bd95-0251-4a51-c8d7-3d0f2cb8dc3f"
      },
      "source": [
        "threshold = 0.5\n",
        "\n",
        "for word, index in word_index_map.items():\n",
        "    weight = model.coef_[0][index]\n",
        "\n",
        "    \n",
        "    if abs(weight) > threshold:\n",
        "        print(word,weight)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bad -0.5902388550978729\n",
            "cable 0.5590829396219031\n",
            "time -0.6739846673734677\n",
            "used 1.0261903891927109\n",
            "'ve 0.6096215416906178\n",
            "month -0.7101215069513618\n",
            "problem 0.6486207532079401\n",
            "need 0.5526452901361916\n",
            "good 1.9025474013793866\n",
            "sound 0.9668409393871448\n",
            "lot 0.6014026624790298\n",
            "n't -1.9868163530762764\n",
            "easy 1.3763757238315055\n",
            "case 0.5237199509002051\n",
            "get -1.0364265164112205\n",
            "use 1.6606862471279287\n",
            "quality 1.1959290852170068\n",
            "best 1.0122222169801935\n",
            "item -0.8750066367075127\n",
            "well 0.9910427956555604\n",
            "wa -1.2003303428726826\n",
            "perfect 0.8226793519102719\n",
            "fast 0.7749987600923057\n",
            "price 2.292341928212701\n",
            "great 3.4272060405239837\n",
            "money -0.9488372084047583\n",
            "memory 0.796381296331678\n",
            "would -0.8056180407442339\n",
            "buy -0.9859340087385279\n",
            "worked -0.7810140343583012\n",
            "pretty 0.5339156834280407\n",
            "could -0.5402350078209843\n",
            "doe -1.0686840615775937\n",
            "two -0.5681898738085115\n",
            "highly 0.8833811788847046\n",
            "recommend 0.5885752419867472\n",
            "first -0.628596440921964\n",
            "customer -0.5288598123431051\n",
            "support -0.7332023448335508\n",
            "little 0.659839433186006\n",
            "returned -0.6470588284906652\n",
            "excellent 1.1197510897826337\n",
            "love 0.8959476166846596\n",
            "small 0.6245276126449657\n",
            "got -0.5107315395859506\n",
            "week -0.5513040554996739\n",
            "using 0.5556716255058628\n",
            "thing -0.8827874030688878\n",
            "also 0.55175558894946\n",
            "even -0.7168403836832621\n",
            "poor -0.654306063206768\n",
            "tried -0.703594584211453\n",
            "back -1.4247997151004765\n",
            "try -0.513782797691581\n",
            "comfortable 0.5282904099227455\n",
            "speaker 0.738749078801771\n",
            "warranty -0.5176643719164987\n",
            "paper 0.5355913424286645\n",
            "return -0.9721549727689738\n",
            "waste -0.8806155271586953\n",
            "refund -0.5295445548911156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8EECB3JLLtn"
      },
      "source": [
        "Notice for the word junk used in reviews, it is more likely to be a negative review. \n",
        "As opposed to a review containing the word 'great' has a large positive weight."
      ]
    }
  ]
}